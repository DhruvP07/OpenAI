{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f61396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abd8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of cities\n",
    "cities = [\n",
    "    \"Austin, TX\",\n",
    "    \"San Antonio, TX\",\n",
    "    \"Dallas, TX\",\n",
    "    \"Houston, TX\",\n",
    "    \"El Paso, TX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00236496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base search query\n",
    "base_query = \"Free photographs of\"\n",
    "\n",
    "# Choose search engine: 'google', 'bing', or 'perplexity'\n",
    "search_engine = 'bing'  # or 'bing' or 'perplexity'\n",
    "\n",
    "# Search engine URL patterns\n",
    "search_urls = {\n",
    "    \"google\": \"https://www.google.com/search?q={}\",\n",
    "    \"bing\": \"https://www.bing.com/search?q={}\",\n",
    "    \"perplexity\": \"https://www.perplexity.ai/search?q={}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7583e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening search: Free photographs of Austin, TX\n",
      "Opening search: Free photographs of San Antonio, TX\n",
      "Opening search: Free photographs of Dallas, TX\n",
      "Opening search: Free photographs of Houston, TX\n",
      "Opening search: Free photographs of El Paso, TX\n"
     ]
    }
   ],
   "source": [
    "def perform_searches():\n",
    "    for city in cities:\n",
    "        query = f\"{base_query} {city}\"\n",
    "        encoded_query = query.replace(\" \", \"+\")\n",
    "        url = search_urls[search_engine].format(encoded_query)\n",
    "        print(f\"Opening search: {query}\")\n",
    "        webbrowser.open_new_tab(url)\n",
    "        time.sleep(2)  # wait a bit before the next search to avoid too many tabs at once\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_searches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3cb2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Unsplash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ac6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Replace with your Unsplash Access Key\n",
    "load_dotenv()\n",
    "\n",
    "key = os.getenv(\"ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18aae48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to save images\n",
    "SAVE_FOLDER = \"downloaded_photos\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# List of cities\n",
    "cities = [\n",
    "    \"Austin, TX\",\n",
    "    \"San Antonio, TX\",\n",
    "    \"Dallas, TX\",\n",
    "    \"Houston, TX\",\n",
    "    \"El Paso, TX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27238cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_unsplash(query):\n",
    "    url = \"https://api.unsplash.com/search/photos\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"client_id\": key,\n",
    "        \"per_page\": 1\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"results\"]:\n",
    "            return data[\"results\"][0][\"urls\"][\"full\"]  # or 'regular' for smaller image\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c6d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download image from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597e2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching: Free photographs of Austin, TX\n",
      "‚úÖ Downloaded: downloaded_photos\\Austin_TX.jpg\n",
      "üîç Searching: Free photographs of San Antonio, TX\n",
      "‚úÖ Downloaded: downloaded_photos\\San_Antonio_TX.jpg\n",
      "üîç Searching: Free photographs of Dallas, TX\n",
      "‚úÖ Downloaded: downloaded_photos\\Dallas_TX.jpg\n",
      "üîç Searching: Free photographs of Houston, TX\n",
      "‚úÖ Downloaded: downloaded_photos\\Houston_TX.jpg\n",
      "üîç Searching: Free photographs of El Paso, TX\n",
      "‚úÖ Downloaded: downloaded_photos\\El_Paso_TX.jpg\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    for city in cities:\n",
    "        query = f\"Free photographs of {city}\"\n",
    "        print(f\"üîç Searching: {query}\")\n",
    "        image_url = search_unsplash(query)\n",
    "        if image_url:\n",
    "            filename = os.path.join(SAVE_FOLDER, city.replace(\", \", \"_\").replace(\" \", \"_\") + \".jpg\")\n",
    "            download_image(image_url, filename)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No image found for {city}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4feb48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29e5b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52e913e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-searchNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading duckduckgo_search-2025.4.4-cp39-cp39-win_amd64.whl.metadata (18 kB)\n",
      "Collecting click>=8.1.8 (from duckduckgo-search)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting primp>=0.14.0 (from duckduckgo-search)\n",
      "  Downloading primp-0.14.0-cp38-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Collecting lxml>=5.3.0 (from duckduckgo-search)\n",
      "  Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhruv\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.6)\n",
      "Downloading duckduckgo_search-2025.4.4-cp39-cp39-win_amd64.whl (60 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.3/3.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.8/3.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.9/3.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading primp-0.14.0-cp38-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.9/3.1 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: primp, lxml, click, duckduckgo-search\n",
      "Successfully installed click-8.1.8 duckduckgo-search-2025.4.4 lxml-5.3.2 primp-0.14.0\n"
     ]
    }
   ],
   "source": [
    "pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eff70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Create folder for images\n",
    "SAVE_FOLDER = \"city_images\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "# List of cities\n",
    "cities = [\n",
    "    \"Austin, TX\",\n",
    "    \"San Antonio, TX\",\n",
    "    \"Dallas, TX\",\n",
    "    \"Houston, TX\",\n",
    "    \"El Paso, TX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f3347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"‚úÖ Downloaded: {filepath}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to download from {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error downloading {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb4d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt\n",
    "# Image of {city} highlighting cultural, sports, major business and entertainment sites.\n",
    "def search_and_download(city):\n",
    "    query = f\"Free photographs of {city}\"\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.images(query, max_results=1)\n",
    "        for result in results:\n",
    "            image_url = result[\"image\"]\n",
    "            filename = city.replace(\", \", \"_\").replace(\" \", \"_\") + \".jpg\"\n",
    "            filepath = os.path.join(SAVE_FOLDER, filename)\n",
    "            download_image(image_url, filepath)\n",
    "            break  # only the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded: city_images\\Austin_TX.jpg\n",
      "‚úÖ Downloaded: city_images\\San_Antonio_TX.jpg\n",
      "‚úÖ Downloaded: city_images\\Dallas_TX.jpg\n",
      "‚úÖ Downloaded: city_images\\Houston_TX.jpg\n",
      "‚úÖ Downloaded: city_images\\El_Paso_TX.jpg\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    for city in cities:\n",
    "        search_and_download(city)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a76fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
